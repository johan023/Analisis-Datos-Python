# -*- coding: utf-8 -*-
"""Practica2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_EikEKs0oIDV96Em4MbfNcJZ3sMeQBiO
"""

# =============================================================================
# Lectura de un fichero
# =============================================================================
#Librería estructura de datos
import pandas as pd

datos = pd.read_csv("votaciones.csv", sep = ",")

# =============================================================================
# Análisis descriptivos preliminares (conociendo nuestros datos)
# =============================================================================
#Descriptiva de los datos
descriptives = datos.describe()


#Valores blancos
print(datos.isnull().sum())
print(pd.isnull(datos).sum())

#Datos únicos que contiene cada variable
datos["Partido Votado"].unique()
datos["Comunidad"].unique()
datos["Edad"].unique()
datos["Ocupacion"].unique()
datos["Genero"].unique()
datos["Estado Civil"].unique()
datos["Patrimonio (euros)"].unique()

#Contar cuántos valores únicos tiene cada variable
datos["Partido Votado"].value_counts()
datos["Comunidad"].value_counts()
datos["Edad"].value_counts()
datos["Ocupacion"].value_counts()
datos["Genero"].value_counts()
datos["Estado Civil"].value_counts()
datos["Patrimonio (euros)"].value_counts()

# =============================================================================
# Gráficos descriptivos. Correlaciones
# =============================================================================
import matplotlib.pyplot as plt
# Gráfico de líneas
plt.plot(datos["Patrimonio (euros)"])
plt.show()
# Histogramas
plt.hist(datos["Ocupacion"], facecolor = 'green')
plt.show()
# Gráfico de dispersión
plt.scatter(datos["Edad"],datos["Patrimonio (euros)"])
plt.show()

import seaborn as sns
sns.pairplot(datos.select_dtypes(exclude=[object]))

import plotly.express as px 
  
df = px.data.iris() 
  
fig = px.box(df, x="sepal_width", y="sepal_length") 
  
fig.show()

import plotly
from plotly.offline import plot
import plotly.graph_objs as go

plotly.offline.plot({
"data": [
    go.Scatter(    x=[1, 2, 3, 4],
    y=[10, 11, 12, 13], mode='markers',
    marker=dict(
        size=[40, 60, 80, 100]))],
"layout": go.Layout(showlegend=False,
    height=600,
    width=600,
)
})

fig = go.Figure(data=[{'type': 'scatter', 'y': [2, 1, 4]}])

plot(fig)

# =============================================================================
# Preprocesamiento
# =============================================================================
#Borrar atributos (variables) que no sean numéricos
#1º opción. Seleccionar solo las variables numéricas y añadir variable objetivo
num_datos = datos.select_dtypes(exclude=[object]) 
num_datos.loc[:, "Genero"] = datos["Genero"]    
num_datos.loc[:, "Estado Civil"] = datos["Estado Civil"]         

#2º opción. Borrar columnas
#num_datos = datos.drop(["A1", "A4", "A5", "A6", "A7", "A9", "A10", "A12", "A13"], axis = 1) #filas 0


#Reemplazar los valores
#num_datos["A2"][83] = 10

#Borrar registros nulos (o vacíos)
num_datos = num_datos.dropna()

#Convertir var. obj. en numérico
num_datos["Genero"].unique()

num_datos["Genero"].unique()
num_datos.loc[num_datos.loc[:, "Genero"] == "M", "Genero"] = 0 
num_datos.loc[num_datos.loc[:, "Genero"] == "F", "Genero"] = 1 

num_datos["Estado Civil"].unique()
num_datos.loc[num_datos.loc[:, "Estado Civil"] == "Soltero", "Estado Civil"] = 0 #Selección de la Class == 'Soltero'
num_datos.loc[num_datos.loc[:, "Estado Civil"] == "Casado", "Estado Civil"] = 1 #Selección de la Class == 'Casado'


num_datos["Estado Civil"].unique()

pd.plotting.scatter_matrix(num_datos, 
                               c = num_datos['Estado Civil'], # color
                               figsize = [10, 10], # tamaño de la ventana   
                               s=35, #tamaño del marcador
                               marker = 'D') # tipo de marcar los puntos

# =============================================================================
# Métodos supervisados. Decission Trees - Sklearn. CLASIFICACIÓN
# =============================================================================
# Importando el árbol de decisión
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt
 
#Selección de las variables
X = num_datos.values[:, :-1] #Variables explicativas
y = num_datos.values[:, -1] #variable objetivo
y = y.astype("int")

# Creando el modelo
model = DecisionTreeClassifier(criterion='entropy', max_depth=4)

# Ajustando el modelo
model = model.fit(X, y)

#Dibujar el árbol entrenado
fig, ax = plt.subplots(figsize=(25, 12)) #Tamaño del gráfico
tree.plot_tree(model, fontsize = 10)


num_datos.columns

# =============================================================================
# Métodos supervisados. Decission Trees - Sklearn. PREDICCIÓN
# =============================================================================
from sklearn.model_selection import train_test_split #Separar el data set en training y test
from sklearn.metrics import accuracy_score #Métricas de la predicción del modelo. Precisión
from sklearn.metrics import confusion_matrix #Métricas de la predicción del modelo. Matriz de confusión

#Selección de las variables
X = num_datos.values[:, :-1] #Variables explicativas
y = num_datos.values[:, -1] #variable objetivo
y = y.astype("int")

#Data sets de training y de test
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

#Crear el modelo
model = DecisionTreeClassifier(criterion='entropy', max_depth=4)

# Ajustando el modelo con el data set de training
model = model.fit(X_train, y_train) 

#Prediciendo sobre el data set de test
y_predict = model.predict(X_test)

#Métrica de precisión en la predicción
accuracy_score(y_test, y_predict)

#Matriz de confusión
pd.DataFrame(
    confusion_matrix(y_test, y_predict),
    columns=['Predicted 0', 'Predicted 1'],
    index=['True Not Class', 'True Class']
)

#Dibujar el árbol entrenado
fig, ax = plt.subplots(figsize=(25, 12)) #Tamaño del gráfico
tree.plot_tree(model, fontsize = 10)


num_datos.columns
# =============================================================================
# Métodos supervisados. Decission Trees - Sklearn. Regresión
# =============================================================================
from sklearn.tree import DecisionTreeRegressor


#Selección de las variables
X = num_datos.values[:, :-1] #Variables explicativas
y = num_datos.values[:, -1] #variable objetivo
y = y.astype("int")

#Data sets de training y de test
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)

#Crear el modelo
model = DecisionTreeRegressor(max_depth=4)

# Ajustando el modelo con el data set de training
model = model.fit(X_train, y_train) 

#Prediciendo sobre el data set de test
y_predict = model.predict(X_test)





#Dibujar el árbol entrenado
fig, ax = plt.subplots(figsize=(25, 12)) #Tamaño del gráfico
tree.plot_tree(model, fontsize = 10)


import numpy as np
from sklearn import metrics

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_predict))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_predict))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_predict)))

# =============================================================================
# Métodos NO supervisados. Clústering - KMeans, Sklearn
# =============================================================================

from sklearn.cluster import KMeans

#Seleccionar los atributos "Edad" y "Patrimonio"
K_datos = num_datos[["Edad", "Patrimonio (euros)"]]

#Crear modelo
kmeans = KMeans(n_clusters = 4)

#Ajustar modelo
kmeans.fit(K_datos)

#Dibujar el clústering
centroids = kmeans.cluster_centers_
labels = kmeans.labels_

#Dibujar los puntos con los colores de cada clúster
colors = ["g.","r.","c.","y."]
for i in range(len(K_datos)):
    plt.plot(K_datos.iloc[i,0], K_datos.iloc[i,1], colors[labels[i]], markersize = 10)
    
#Dibujar los centroides de cada clúster
plt.scatter(centroids[:, 0],centroids[:, 1], marker = "x", s=150, linewidths = 5, zorder = 10)

plt.show()



K_datos["cluster"] = labels


summary0 = K_datos[K_datos.cluster == 0].describe()
summary1 = K_datos[K_datos.cluster == 1].describe()
summary2 = K_datos[K_datos.cluster == 2].describe()
summary3 = K_datos[K_datos.cluster == 3].describe()

####### Reglas de asociación 

!pip install apyori
 
import numpy as np 
import pandas as pd

datos = datos[["Partido Votado", "Comunidad", "Ocupacion", "Genero"]]

# Intializing the list
transacts = []
# populating a list of transactions
for i in range(0, 100): 
  transacts.append([str(datos.values[i,j]) for j in range(0, 3)]) # la ultima columna 3

from apyori import apriori
rule = apriori(transactions = transacts, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2, max_length = 2)



output = list(rule) # returns a non-tabular output
# putting output into a pandas dataframe
def inspect(output):
    lhs         = [tuple(result[2][0][0])[0] for result in output]
    rhs         = [tuple(result[2][0][1])[0] for result in output]
    support    = [result[1] for result in output]
    confidence = [result[2][0][2] for result in output]
    lift       = [result[2][0][3] for result in output]
    return list(zip(lhs, rhs, support, confidence, lift))
output_DataFrame = pd.DataFrame(inspect(output), columns = ['Left_Hand_Side', 'Right_Hand_Side', 'Support', 'Confidence', 'Lift'])



output_DataFrame